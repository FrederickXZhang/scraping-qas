# Scraping Frequently Asked Questions

This subdirectory deals with webscraping Frequently Asked Questions (FAQs). The goal is to scrape data from trusted source and store the data in our [schema](https://github.com/jsedoc/Covid-19-infobot/wiki/Schema-v0.1). Another group will then deal with the NLP to make this data useful.

## Setup

Run `conda env create -f environment.yml` to setup the conda environment with the correct configurations.
This project uses python3.6. 

Make sure to then run `python setup.py install`. This will create a local library called `covid_scraping` that you will use.

### Installing dependenceis

Use conda to install dependencies

### Updating the conda env

If you installed new dependencies, run `conda env export --from-history --ignore-channel > environment.yml.tmp`.
Now, merge `environment.yml.tmp` and `environment.yml` into `environment.yml` so that you do not overwrite other
dependencies in the yml file.
Finally, push the new `environment.yml` configuration file


## Websites to scrape

We have a list of websites to scrape. Please choose one of the websites from our [todo list](https://github.com/jsedoc/Covid-19-infobot/projects/3).

### Scraping a new website
Once you have claimed a website to work on, move it from the todo column to the in progress column on our [board](https://github.com/jsedoc/Covid-19-infobot/projects/3) and assign yourself to the issue corresponding to the website.

Next, create a new branch using
`git checkout -b <name-of-new-branch` where the branch name should be `scraping-<name of website`.
You will implement your scraper in a new file in https://github.com/jsedoc/Covid-19-infobot/tree/master/src/scraping/scrapers.
Please name the new file the name of the website you are scraping, so if you are scraping FAQs from the World Health Organization, the filename should be `who.py`. 

#### Implementing Scraper clasc
All your code needs to do is implement the [Scraper abstract class](https://github.com/jsedoc/Covid-19-infobot/blob/2f427fa618873e7e2025bdb86bd8bfdaf2fd61b2/src/scraping/covid_scraping/scraper.py#L17-L31).

Look at [example_scraper](https://github.com/jsedoc/Covid-19-infobot/blob/master/src/scraping/scrapers/example_scraper.py) on how to implement the `scrape()` function.

**TODO for ADAM**: update instructions on scraping


#### Code styling
Before you are finished, make sure that your code abides by our coding style. We use standard [pep8](https://www.python.org/dev/peps/pep-0008/). Run `pep8 <python file name>`. Please fix all style comments (except for line length, and "module level import not at top of file").

Once you get to this point, please make a pull request and assign the pull request to @azpoliak.

## Re-running scrapers
At least once a day, we will re-run all the developed scrapers to added updated FAQs from each scraper.

NOTE: You'll be running this from the master branch. So to setup, clone the master repository. Then, follow the setup instructions at the top of this README file. That is, make sure you're in an updated conda environment (e.g. `conda env update --file environment.yml`) and that you succesfully run `python setup.py install` without any errors.

### Scraping and Pushing Updated Data.
### 1. Internal QAs
We have a list of internal Questions and Answers written by researchers at JHU Public Health. The list can be found in these [google spreadsheet](https://docs.google.com/spreadsheets/d/1Drmwo62V4MvB1X6eTwi1L-f3EYq09oocQ2Jvo-XR1TQ/edit?usp=sharing) To convert those questions and answers follow these steps:

1. download the `Info` and `Questions from Turkle` tabs as tsv files
2. store them in `scrapers/` as `COVID19infosheet - Info.tsv` and `COVID19infosheet - Questions from Turkle .tsv` respectively.
3. cd into `scrapers/` and run `python internalQAs_scraper.py`

### 2. Run the other scrapers
This will be done by running `scrapers/scrape_all.py` with e.g. `python scrape_all.py`

### 3. Run Deepset scraper separately
Change the path argument in main to `../../../data/scraping/`
Run `python deepsetAI_scraper.py` 

### 4. Push the new data (JSONL)
At this step, you can push the updated JSONL files to master. Example commit message: Uploaded updated data. Then, go the commit you just made on GitHub and tag Tongfei, Eddie, Evans in the commit conversation and tell them that you pushed the updated data.

### Making the data public.
From `scrapers/`, run `python make_public.py --path ../../../data/scraping/schema_v0.2/`.
This will create a tsv file [scraped.tsv](https://github.com/jsedoc/Covid-19-infobot/blob/master/data/scraping/schema_v0.2/scraped.tsv) in that path that is ready to be released. 
Next, update the [README_public.md](https://github.com/jsedoc/Covid-19-infobot/blob/master/data/scraping/schema_v0.2/README_public.md) based on the output of the `make_public.py` script.
This tsv file does not include the internal data that was generated by Smisha and co.



